\section{Union-Find Node-Suspension decoder}\label{sec:ufbb}

In this section, we describe the \emph{Union-Find Node-Suspension} decoder, which increases the Union-Find decoder's performance by improving its heuristic for minimum-weight matching. We first introduce the concept of the potential matching weight in \Cref{sec:matchingweight}. We describe the data structure required for this decoder in \Cref{sec:nodeset}, and the necessary calculations performed on this data structure in \Cref{sec:paritydelaysus,sec:nodejoin,sec:inversion}. The pseudocode is included in \Cref{sec:pseudocode}. 

\Figure[htb](topskip=0pt, botskip=0pt, midskip=0pt){tikzfigs/tikz-figure0.pdf}{
    A cluster with vertices $\{v_0, v_1, v_2\}$ with potential matching weights $\{2, 3, 2\}$. The line style and color of the colored edges correspond to the matching in the hypothetical union with an external vertex $v'$ of the same line style and color.\label{fig0}}

\subsection{Potential matching weight}\label{sec:matchingweight}
%In the following we give some intuition into the improvement of the Union-Find Balanced Bloom decoder upon the original Union-Find decoder. 
% We compared the ratio of the matchings between the MWPM decoder and our own implementation of the UF decoder, averaged over many simulations, and found that UF matching weight has a constant prefactor of $\sim 1.043$ over the minimum weight for the toric code (\Cref{comp_weight}). From this, we suspected that a decreased matching weight is a heuristic for an increased threshold. Within the context of the UF decoder, the matching weight may be decreased by prioritizing the growth of vertices with low PWM's within the cluster. 

Consider the cluster with index $i$ containing the set of non-trivial vertices $V_i=\{v_0,v_1,v_2\}$ and set of edges $E_i=\{(v_0,v_1), (v_1, v_2)\}$ of \Cref{fig0}. Now let us investigate the weight of a matching if an additional non-trivial vertex $v'$ is connected to the cluster. If $v'$ is connected to $v_0$ or to $v_2$, then the resulting matching has a total weight of 2: $(v',v_0)$ and $(v_1,v_2)$, or $(v_0,v_1)$ and $(v_2,v')$. However, if $v'$ is connected to vertex $v_2$, then the total weight is 3: $(v', v_1)$ and $(v_0, v_2)$. Inspired by this idea, we introduce the concept of potential matching weight (PMW) of a vertex. 

\begin{definition}\label{def:pmw}
    % For, the hypothetical merger with another odd-parity cluster $V_j, E_j$ on the edge $(v_i, v_j)$, with $v_i\in V_i$ and  $v_j \in V_j$, outputs an even-parity cluster with edges $E_{ij} = E_i \cup E_j \cup (v_i, v_j)$ in which there exists a matching $\m{C}_{(v,v')} \subseteq E_{ij}$ between syndromes internal to the cluster.
    % Let the Potential Matching Weight (PMW) of vertex $v_\alpha \in V_\alpha$ in an odd-parity cluster $\alpha$ with vertices $V_\alpha$ and edges $E_\alpha$ be
    % \begin{equation}
    %   PMW(v_\alpha) = \abs{\m{C}_{(v_\alpha,v_\beta)} \cap E_\alpha} + 1,
    % \end{equation}
    % where matching $\m{C}_{(v,v')} \subseteq E_{ij}$ is between the syndrome vertices internal to the even-parity cluster with edges $E_{ij} = E_\alpha \cup E_\beta \cup (v_\alpha, v_\beta)$, after a hypothetical merger of cluster $\alpha$ with another odd-parity cluster $V_\beta, E_\beta$ on the edge $(v_\alpha, v_\beta)$, with $v_\alpha\in V_\alpha$ and  $v_\beta \in V_\beta$
    Let there be a hypothetical merger between odd cluster $\alpha$ of vertices $V_\alpha$ and edges $E_\alpha$, and odd cluster $\beta$ of $V_\beta$ and $E_\beta$, on the edge $(v_\alpha, v_\beta)$, where $v_\alpha \in V_\alpha$ and $v_\beta \in V_\beta$. In the merged even cluster with edges $E_{\gamma} = E_\alpha \cup E_\beta \cup (v_\alpha, v_\beta)$, there is a matching $\m{C}_{(v,v')} \subseteq E_{\gamma}$  between the syndrome vertices internal to the cluster. The \textbf{Potential Matching Weight} (PMW) of vertex $v_\alpha$ is then defined as
    \begin{equation}
      PMW(v_\alpha) = \abs{\m{C}_{(v_\alpha,v_\beta)} \cap E_\alpha} + 1.
    \end{equation}
\end{definition}

In other words, the PMW is a vertex-specific predictive heuristic to the matching weight, assuming a union occurs in the next growth iteration. The PMW can be utilized by prioritizing the growth of vertices with low PMW such that there is an increased probability of mergers between clusters on edges connected to these vertices, and there is an increased probability in a lower matching weight. However, the PMWs' calculation within a cluster is not a trivial task, especially for clusters of increasingly larger size, as all edges of a cluster must be considered in its calculation. Furthermore, the PMWs within a cluster change due to cluster growth and mergers, both of which occur more frequently as the system size increases. For this reason, the scaling of the PMW computation is vital to the decoder. 

\Figure[htb](topskip=0pt, botskip=0pt, midskip=0pt){tikzfigs/tikz-figure1.pdf}{
    The cluster of \Cref{fig0} after two rounds of prioritized growth of $v_0$ and $v_2$. There are regions of vertices that are either interior elements or have equal potential matching weights, represented as nodes with different node radii in the node-tree $\nset$. \label{fig1}}

\subsection{Node-Suspension data structure}\label{sec:nodeset}

Fortunately, the PMW calculation is quite efficient by the introduction of a new data structure. Consider the cluster of non-trivial vertices $V_i=\{v_0,v_1,v_2\}$ and edges $E_i = \{(v_0,v_1), (v_1, v_2)\}$ from \Cref{fig0}. We had found previously that vertices $v_0, v_2$ have a lower PMW compared to $v_1$ by 1 edge. The growth of $v_0$ and $v_2$ are thus prioritized, such that new vertices are added to the cluster on the boundary of $v_0$ and $v_2$. If all newly added vertices are trivial, the cluster is now as in \Cref{fig1}. If we repeat the PMW calculation, we now find that the PMWs in the new vertices connected to $v_0$ are equal. The same is true for vertices connected to $v_2$. 
\begin{definition}\label{def:vertextree}
    Let the vertex-tree $\vset_i$ be a connected acyclic subgraph of the graph of a cluster $G(V_i, E_i)$.   The vertex-tree $\vset_i$ includes all vertices $V_i$ and a minimum number of edges in $E^\vset_i \subseteq E_i$. 
\end{definition}
\begin{definition}
  Let the node-tree $\nset_i$ be a partition of the vertex-tree $\vset_i$, such that each element of the partition --- a \textbf{node} $n$ --- consists of a set of adjacent vertices that lie within a certain distance --- the \textbf{node radius} $r$ --- from the \textbf{primer vertex}, which initializes the node and lies at its center. The node-tree is a directed acyclic graph, and its edges $\m{E}_i$ have lengths equal to the distance between the primer vertices of neighboring nodes. 
\end{definition}

The concept of primer vertices is easily understood when considering non-trivial vertices of the syndrome $\sigma$. Suppose every non-trivial vertex is the primer of a node, the weight of a matching in $\vset_i$ equal to the weight of the same matching in $\nset_i$. Furthermore, for every node of the node-tree, all vertices that lie at distance $r$ to the primer vertex are either boundary vertices to the cluster and have equal PMW, or lie within the radius of another node. For the example in \Cref{fig1}, the PMW of all boundary vertices of $n_0$, for simplicity just the PMW of $n_0$, is $\floor{r_0} + (n_1, n_2) + 1$. The partition from $\vset$ to $\nset$ thus allows us to compute the PMW on a reduced tree. 

\Figure[htb](topskip=0pt, botskip=0pt, midskip=0pt){tikzfigs/tikz-figure2.pdf}{
    Two different types of nodes. Syndrome-nodes $s$ have a non-trivial vertex or syndrome at its center. Vertices that lie on the radii of two existing nodes initialize a junction-node $j$ in the node-tree.\label{fig2}}

\Figure[hbt](topskip=0pt, botskip=0pt, midskip=0pt){tikzfigs/tikz-figure3.pdf}{
    The relevant data structures. \emph{(a)} The cluster-tree of the Union-Find data structure. The path from a vertex to the root of the cluster-tree is traversed to find the root element in order to differentiate between clusters. The root node of the node-tree is now additionally stored at the root of the cluster-tree. \emph{(b)} The vertex-tree $\vset$ with 9 non-trivial vertices. As $\vset$ is strictly acyclic, the cluster's edges must be maintained such that no cycles are created. This is done during growth by removing edges (red dotted lines) if a cycle is detected. \emph{(c)} The node-tree $\nset$, which currently has the same number of elements as $\vset$, as all vertices are non-trivial. Two depth-first searches are required to compute node parities (head recursively) and delays (tail recursively) in $\nset$.\label{fig3}}

All non-trivial vertices serve as primers for nodes that are called \textbf{syndrome-nodes} $s$. However, not all primer vertices are non-trivial vertices of the syndrome. If two non-trivial vertices are located an even Manhattan distance on the lattice, the growth of their clusters can simultaneously reach some vertex that lies on equal radii of the associated nodes, such as in \Cref{fig2}. For this reason, such vertices serve as primers of a different type of node --- a \textbf{junction-node} $j$ --- in the merged node-tree. 

The calculation of the PMW on the node-tree $\nset$ rather than the vertex-tree $\vset$ offers a reduction in the cost. However, it is still no trivial task as the entire tree must be considered for the calculation in every node. Instead, we will compute for the \textbf{node suspension} $n_s$ --- the number of growth iterations needed for a node to reach the maximum PMW in the node-tree --- which relates closely to the PMW. For example, the node suspension for the nodes $\{n_0, n_1, n_2\}$ associated with the vertices $\{v_0, v_1, v_2\}$ in \Cref{fig0} is $\{0, 2, 0\}$, and $\{0, 1, 0\}$ in \Cref{fig1}.

The Node-Suspension data structure does not replace but coexists with the Union-Find data structure. Additional to the the Union-Find data structure's cluster-trees of distinct roots, we store for every cluster the node-tree $\nset_i$ by its root node. For this, we need to maintain the reduced set of edges $E^\vset_i \subseteq E_i$ of the vertex-trees $\vset_i$ for every cluster, which can be done in constant time (see Algorithm \ref{algo:ufbb}). In the UF decoder, vertex-trees $\m{V}_i$ are not maintained, such that the graph associated with each cluster is not acyclic \cite{delfosse2017almost}. Instead, a spanning forest $F$ of all clusters is created \cite{delfosse2017linear} after growth. Each connected element within $F$ is also an acyclic graph. The difference is that while a single depth-first search or breath-first-search creates $F$, $\vset$ is equivalent to multiple breadth-first searches from each non-trivial vertex within the cluster, where the search of every breadth occurs during a growth iteration. The relevant data structures are depicted in \Cref{fig3}. 


\subsection{Node parity, delay, and suspension}\label{sec:paritydelaysus}

The Node-Suspension data structure allows for calculating the node suspension of all nodes in a node-tree $\nset$ by two intermediate steps. In each step, a depth-first-search (DFS) of $\nset$ is applied from its root node $r$ (\Cref{fig3}c).

In the first DFS, we calculate for the \textbf{node parity} $n_p$ --- the number of descendant syndrome-nodes of a node modulo 2 --- via a tail-recursive function, which is only dependent on the node parities of the children nodes of a node. The node parity is defined differently per node type:
\begin{align}\label{eq:nodeparity}
    s_p &= \hspace{.6cm}\big( \sum_{\mathclap{n \in \text{ children of } s}} (1-n_p) \big ) \bmod 2,\\
    j_p &= 1 - \big(\sum_{\mathclap{n \in \text{ children of } j}} (1-n_p) \big) \bmod 2.
\end{align}

In the second DFS, we calculate for the difference in node suspension of a node $n$ with its parent $m$; $\delta = n_s - m_s$. We can choose an arbitrary \textbf{node delay} $n_d$ --- the node suspension minus the maximum node suspension in the node-tree --- for the root node $r$ such as $r_d=0$ and add the suspension difference $\delta$ during each step to obtain $n_d$ for every node. This node delay of a node $n$ is only dependent on the node radii of itself and its parent $m$, the length of edge $(n,m)$, and its parity $n_p$. 
\begin{multline}\label{eq:delayequation}
    n_d = m_d + \bigg \lceil 2C\big(\ceil{n_r} - \floor{m_r + n_r \bmod 1}\\
    - (-1)^{n_p}\abs{(n,m)}\big) - 2(n_r - m_r) \bmod2 \bigg \rceil
\end{multline}
Here, the \textbf{inversion constant} $C$ deals with the inversion of node parities in a node-tree during merges of clusters explained in \ref{sec:nodejoin}. The node suspension is then related to the node delay by
\begin{equation*}
    n_s = n_d - \max_{x \in \nset}{x_d}. 
\end{equation*}
The maximum node delay can be maintained during the second DFS of the node-tree, and the node suspension itself is calculated during cluster growth. A single growth iteration, which is applied in the UF decoder by adding half-edges to all boundary vertices of the cluster, is now replaced by another DFS of $\nset$. During this DFS, we calculate the suspension $n_s$ for a node, and conditionally grow it - adding half-edges to the boundary vertices in the current node and adding 1 to its radius $n_r$ --- if $n_s = 0$. This requires us to save the list of boundary vertices to each node (\Cref{fig3}c). When all $n_s$ in $\nset$ are zero, all nodes are grown simultaneously within the same iteration. 

If the node-tree does not change after a growth iteration, which is the case if no mergers occur between clusters, the node suspensions decrease in an expected manner: For all nodes that are not suspended from growth, their node suspensions decrease with 1 in the next growth iteration. Due to this behavior, we can reuse the node delays $n_d$ to calculate $n_s$ for the next growth iteration by introducing another node parameter $n_w$, the number of iterations a node has \textbf{waited}. Each time a node is suspended from growth, we add 1 to $n_w$. The node suspension in subsequent iterations is then
\begin{equation}\label{eq:suspension}
    n_s = n_d - \max_{x \in \nset}{x_d} - n_w. 
\end{equation}
Note that we have not stated which node in $\nset$ should be the root node. In fact, any node in $\nset$ could have been picked as the root of the node-tree. As long as the DFS of cluster growth is performed in the same direction as the DFSs of the parity and delay calculations. If no cluster mergers occur, the node delays can be reused in the node suspension calculation prior to node growth. 
The node-tree is constructed by storing all neighbors of a node to a list. This way, the DFSs' direction can be determined by simply saving the root node, the starting point of the DFSs, to the cluster. All node variables are depicted in \Cref{fig3}c. 
%In the next section, we expand upon this idea of "reusing" some intermediate parameters to calculate the node suspensions after a cluster merger.  


\subsection{Joining node-trees}\label{sec:nodejoin}

In the Union-Find (UF) algorithm, odd parity clusters of an odd number of non-trivial vertices, --- elements of $\sigma$ --- grow in size repeatedly and merge with other clusters until all clusters are even. During these mergers, the node-trees of the Node-Suspension data structure must also be combined. Let us now first make a clear distinction between the merging protocols of the underlying data structures; the clusters-trees of the UF data structure are merged with the \codefunc{Union} function, whereas the node-trees are merged with a separate \codefunc{Join} function. After a join of multiple node-trees, the node suspensions within the combined node-tree change. Therefore, \codefunc{Join} protocol's focus is to minimize the DFSs of the recalculation of the node parity and delays in the combined node-tree. 

First, note that as a cluster of even parity has an even number of non-trivial vertices, its node-tree has an even number of syndrome-nodes. For these even node-trees, the concept of PMW does not exist, as the matching can be made within the node-tree. Consequently, node suspension, parity, and delays are undefined when two odd node-trees join to an even node-tree. 
%Thus, if two odd clusters merge into an even cluster, we don't know and do not care about its node suspensions. 

\Figure[hbt](topskip=0pt, botskip=0pt, midskip=0pt){tikzfigs/tikz-figure4.pdf}{
    \emph{(a)} An odd cluster $\nset_o=\{n_1, n_2, n_o\}$ with root $n_1$ joins with an even cluster $\nset_e=\{n_3, n_e\}$ with root $n_3$ on nodes $n_o, n_e$, respectively, to a joined node-tree. If we choose to \emph{(b)}, make $n_e$ a child of $n_o$, the parities and delays the sub-tree of $\nset_o$ can unchanged, and we only have to perform partial parity and delay calculations over the sub-tree of $\nset_e$. If we choose to \emph{(c)}, make $n_o$ a child of $n_e$, parities and delays have to be recalculated in the entire joined node-tree. \label{fig4}}

The second type of merger is between an even and an odd cluster. The combined cluster is odd, and its growth is continued. Thus its node suspensions must be computed. Consider the example of odd node-tree $\nset_o$ and even node-tree $\nset_e$ that are to be joined on nodes $n_o\in \nset_o$ and $n_e \in \nset_e$ (\Cref{fig4}\emph{a}). If $\nset_o$'s root is kept as the root of the joined node-tree (\Cref{fig4}\emph{b}), $n_e$ is to be a child node of $n_o$. As $\nset_e$ contains an even number of syndrome-nodes, the node parities in $\nset_o$ do not change. Hence, the node parity DFS is only necessary in the sub-tree $\nset_e$, which now has $n_e$ as sub-root. Furthermore, as the node delay is only dependent on its own properties and its parent's, the node delay DFS is also only required from node $n_e$ and within the sub-tree of $\nset_e$. These so-called \textbf{partial} DFSs of the node-tree are precisely what was required, as the node parity and delays in $\nset_e$ were undefined. Alternatively, if $\nset_e$'s root becomes the root of the combined tree (\Cref{fig4}\emph{c}), an odd number of syndrome-nodes are attached to $n_e$, such that the parities of nodes on the path from $n_e$ to the root are changed. Such a join would require the DFSs on the entire combined node-tree to calculate for node parities and delays. Thus, a simple rule is always to keep the root of the odd node-tree, which we dub \textbf{Odd-Rooted Join}.

In addition, a cluster can be subjected to multiple mergers within the same growth iteration, during which the parity of the merged cluster changes dependent on the number of mergers and the parities of the clusters involved. The DFSs related to the parity and delay calculations must, for this reason, not be initiated directly after the joining of node-trees. After all, it may be possible for the cluster to merge again such that the parities and delays become invalid. To prevent these redundant calculations, sub-roots of the even sub-trees are stored to a list $\m{S}$ at the root of the node-tree (\Cref{fig3}\emph{c}). When multiple mergers occur, the root node that stores the now redundant sub-roots is replaced by a new root with new $\m{S}$. If a cluster is selected for growth, we check for the sub-roots in $\m{S}$ at the new root node and initiate the DFSs from these sub-roots. We call this the \textbf{Root List $\m{S}$ Replacement}. 

\Figure[htb](topskip=0pt, botskip=0pt, midskip=0pt){tikzfigs/tikz-figure5.pdf}{
    The node suspension values for nodes for 3 odd node-trees $\{\nset_1, \nset_2, \nset_3\}$ of 3 nodes that grow and join into a single node-tree. \emph{(a)} Node suspensions are calculated by setting $C=1$ in equation \eqref{eq:delayequation}. In step 1, the growth in each of the three node-trees' outer nodes is prioritized, and the node-trees merge. In step 2, the recalculation of the joined node-tree is performed. Parities within the sub-tree of $\nset_2$ are now inverted, and the suspension in these nodes have doubled. \emph{(b)} Node suspensions are calculated by setting $C=\nicefrac{1}{2}$. Now the increase in node suspensions after parity inversion is halved.\label{fig5}}

\subsection{Parity inversion}\label{sec:inversion}
An unfortunate effect of the Node-Suspension data structure, which we dub \textbf{Parity Inversion}, causes a decrease in the algorithm's performance as the lattice size is increased. We will demonstrate this effect through the example in \Cref{fig5}\emph{a}. Consider three instances of the node-tree of \Cref{fig0}; $\nset_a, \nset_b, \nset_c$, positioned near each other on the lattice. For each node-tree, if the middle node is suspended from growth for two iterations, all nodes have the same Potential Matching Weight. However, in the current example, the node-trees $\nset_a, \nset_b, \nset_c$ merge after 1 iteration. The combined node-tree is odd. Thus, we recalculate the node parities and delays to find that the parities in the partition of the node-tree containing the nodes of $\nset_b$ have been inverted, and the node suspensions in this partition have doubled from before node suspensions before the merger. If the next merging event occurs on the node with the doubled node suspension, the matching weight may be larger compared to the original UF decoder, which defies the goal of Node-Suspension to decrease the weight.

This defines a trade-off in the Node-Suspension data structure; a node must wait as many iterations as it is suspended to reach equilibrium in Potential Matching Weight in the node-tree, but after Parity Inversion, the node suspension for previously prioritized nodes increases linearly with the number of iterations waited by the suspended nodes pre-inversion. As a compromise, we redefine the node suspension as \textbf{half} the number of growth iterations needed for all nodes in the node-tree to reach equal PMW. This can be done by setting $C=0.5$ in Equation \eqref{eq:delayequation}. Nevertheless, as more inversions occur, the maximum node suspension in the node-tree increases, and it becomes more and more unlikely for a cluster to actually reach zero node suspension in all nodes. The number of inversions is directly related to the number of merging events, and thus the size of the lattice. The performance to improve the heuristic for minimum weight matching thus decreases for larger lattices. 

